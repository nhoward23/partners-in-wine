{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Project\n",
    "## Red Wine Quality\n",
    "\n",
    "This dataset is various attributes describing the chemical makeup of the wine Portuguese \"Vinho Verde\" wine. for varying seasons.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "import utils\n",
    "\n",
    "table = utils.read_table('red_wine_quality.csv')\n",
    "header = table[0]\n",
    "table = table[1:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K-Means Clustering \n",
    "\n",
    "We can use k-means clustering to determine what attributes make a wine good quality. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13026.327188734273\n",
      "[[7.79, 0.58, 0.25, 2.99, 0.09, 22.66, 97.17, 1.0, 3.31, 0.63, 10.11, 5.38], [8.15, 0.53, 0.24, 2.26, 0.08, 13.25, 32.42, 1.0, 3.34, 0.64, 10.5, 5.73], [8.46, 0.54, 0.27, 2.51, 0.1, 14.49, 52.21, 1.0, 3.3, 0.69, 10.2, 5.53], [8.05, 0.56, 0.33, 3.46, 0.09, 32.45, 139.35, 1.0, 3.24, 0.71, 9.81, 5.13], [7.92, 0.51, 0.24, 2.32, 0.08, 28.85, 48.93, 1.0, 3.36, 0.69, 10.65, 5.81], [8.3, 0.51, 0.3, 2.8, 0.09, 26.9, 70.87, 1.0, 3.32, 0.67, 10.23, 5.54], [8.75, 0.51, 0.3, 2.47, 0.09, 6.53, 16.55, 1.0, 3.29, 0.65, 10.65, 5.74]]\n",
      "Random Instance:  [8.9, 0.22, 0.48, 1.8, 0.077, 29, 60, 0.9968, 3.39, 0.53, 9.4, 6]\n",
      "Majority Classification:  5.0\n",
      "============================================================\n",
      "values:  [4.0, 5.0, 6.0, 7.0, 8.0]\n",
      "counts:  [3, 110, 37, 11, 2]\n",
      "average vals: [7.79, 0.58, 0.25, 2.99, 0.09, 22.66, 97.17, 1.0, 3.31, 0.63, 10.11, 5.38]\n",
      "============================================================\n",
      "============================================================\n",
      "values:  [3.0, 4.0, 5.0, 6.0, 7.0, 8.0]\n",
      "counts:  [1, 10, 145, 188, 52, 4]\n",
      "average vals: [8.15, 0.53, 0.24, 2.26, 0.08, 13.25, 32.42, 1.0, 3.34, 0.64, 10.5, 5.73]\n",
      "============================================================\n",
      "============================================================\n",
      "values:  [3.0, 4.0, 5.0, 6.0, 7.0, 8.0]\n",
      "counts:  [2, 7, 85, 76, 16, 1]\n",
      "average vals: [8.46, 0.54, 0.27, 2.51, 0.1, 14.49, 52.21, 1.0, 3.3, 0.69, 10.2, 5.53]\n",
      "============================================================\n",
      "============================================================\n",
      "values:  [4.0, 5.0, 6.0, 7.0]\n",
      "counts:  [1, 59, 6, 2]\n",
      "average vals: [8.05, 0.56, 0.33, 3.46, 0.09, 32.45, 139.35, 1.0, 3.24, 0.71, 9.81, 5.13]\n",
      "============================================================\n",
      "============================================================\n",
      "values:  [3.0, 4.0, 5.0, 6.0, 7.0, 8.0]\n",
      "counts:  [1, 4, 39, 68, 22, 1]\n",
      "average vals: [7.92, 0.51, 0.24, 2.32, 0.08, 28.85, 48.93, 1.0, 3.36, 0.69, 10.65, 5.81]\n",
      "============================================================\n",
      "============================================================\n",
      "values:  [4.0, 5.0, 6.0, 7.0, 8.0]\n",
      "counts:  [4, 77, 76, 7, 1]\n",
      "average vals: [8.3, 0.51, 0.3, 2.8, 0.09, 26.9, 70.87, 1.0, 3.32, 0.67, 10.23, 5.54]\n",
      "============================================================\n",
      "============================================================\n",
      "values:  [3.0, 4.0, 5.0, 6.0, 7.0, 8.0]\n",
      "counts:  [6, 24, 166, 187, 89, 9]\n",
      "average vals: [8.75, 0.51, 0.3, 2.47, 0.09, 6.53, 16.55, 1.0, 3.29, 0.65, 10.65, 5.74]\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import math\n",
    "import copy\n",
    "import numpy\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def compute_distance(v1, v2):\n",
    "    \"\"\"computes the distance between v1 and v2 using Eucildean distance. Does not include the classification attribute.\"\"\"\n",
    "    #assert(len(v1) == len(v2))\n",
    "    dist = math.sqrt(sum([(v1[i] - v2[i]) ** 2 for i in range(len(v2)-1)]))\n",
    "    return dist\n",
    "\n",
    "def select_init_centroids(k, table):\n",
    "    \"\"\"select k instances at random to use as the initial centroids.\"\"\" \n",
    "    init_centroids = []\n",
    "    for i in range(k):\n",
    "        instance_index = random.randrange(len(table))\n",
    "        while table[instance_index] in init_centroids:\n",
    "            instance_index = random.randrange(len(table))\n",
    "        init_centroids.append(table[instance_index])\n",
    "    return init_centroids\n",
    "\n",
    "def append_distance(table, distances, point):\n",
    "    \"\"\"appends the distance between row in table and point to distances.\"\"\"\n",
    "    for i, row in enumerate(table):\n",
    "        distance = compute_distance(row, point)\n",
    "        distances[i].append(distance)\n",
    "        \n",
    "def find_centroid(distances):\n",
    "    \"\"\"gets the centroid index of smallest distance and appends it to distances.\"\"\"\n",
    "    for row in distances:\n",
    "        cluster_index = row.index(min(row))\n",
    "        row.append(cluster_index)\n",
    "\n",
    "def combine_tables(table1, table2):\n",
    "    \"\"\"combines table 1 and table 2 together.\"\"\"\n",
    "    new_table = []\n",
    "    assert(len(table1) == len(table2))\n",
    "    for i, row in enumerate(table1):\n",
    "        new_row = row + table2[i]\n",
    "        new_table.append(new_row)\n",
    "    return new_table\n",
    "\n",
    "def compute_average(cluster, atts_range):\n",
    "    \"\"\"atts_range: is the range [0-att_range) in which to compute averages.\"\"\"\n",
    "    centroid = []\n",
    "    for i in range(atts_range):\n",
    "        column = utils.get_column(cluster, i)\n",
    "        att_average = numpy.mean(column)\n",
    "        centroid.append(round(att_average,2))\n",
    "    return centroid\n",
    "        \n",
    "def recalculate_centroids(table, distances_table, att_list):\n",
    "    # Combine and group by cluster \n",
    "    new_table = combine_tables(table, distances_table)\n",
    "    cluster_index = len(new_table[0])-1\n",
    "    group_names, groups = utils.group_by(new_table, cluster_index)\n",
    "    \n",
    "    # For each cluster\n",
    "    new_centroids = []\n",
    "    for cluster in groups:\n",
    "        # get the average of each attribute\n",
    "        new_centroid = compute_average(cluster, len(table[0]))\n",
    "        new_centroids.append(new_centroid)\n",
    "    return new_centroids\n",
    "    \n",
    "def compare_centroids(centroids1, centroids2):\n",
    "    assert(len(centroids1) == len(centroids2))\n",
    "    for i in range(len(centroids1)):\n",
    "        if centroids1[i] != centroids2[i]:\n",
    "            return False\n",
    "    return True \n",
    "\n",
    "def k_means_clustering(k, table):\n",
    "    # Select k objects in an arbitrary fashion. Use these as the initial set of k centroids.\n",
    "    centroids = select_init_centroids(k, table)\n",
    "    \n",
    "    match = False\n",
    "    while match == False: \n",
    "        \n",
    "        # Compute the distances of each instance to each centroid\n",
    "        distances_table = [ [] for i in range(len(table)) ]\n",
    "        for centroid in centroids:\n",
    "            append_distance(table, distances_table, centroid)\n",
    "\n",
    "        # Find the biggest distance and assign instance to that centroid\n",
    "        find_centroid(distances_table)\n",
    "\n",
    "        # Recalculate the centroids by getting the mean of each cluster\n",
    "        new_centroids = recalculate_centroids(table, distances_table, [])\n",
    "\n",
    "        # Check to see if the centroids have converged\n",
    "        match = compare_centroids(centroids, new_centroids)\n",
    "        centroids = new_centroids\n",
    "\n",
    "    # Now we know what instance belongs to what cluster\n",
    "    # these are \"parallel tables\" \n",
    "    score = objective_function(table, distances_table, centroids)\n",
    "    return score, distances_table, centroids\n",
    "    \n",
    "def objective_function(table, cluster_table, centroids):\n",
    "    # Combine and group by cluster \n",
    "    new_table = combine_tables(table, cluster_table)\n",
    "    cluster_index = len(new_table[0])-1\n",
    "    group_names, groups = utils.group_by(new_table, cluster_index)\n",
    "    \n",
    "    # for each cluster, compute the sum of squares\n",
    "    # add these to a cluster_total of all clusters\n",
    "    total_cluster_score = 0\n",
    "    for i, cluster in enumerate(groups):\n",
    "        distances = []\n",
    "        for row in cluster:\n",
    "            distance = compute_distance(row, centroids[row[cluster_index]])\n",
    "            distances.append(distance)\n",
    "        total_cluster_score += sum(distances)\n",
    "    print(total_cluster_score)\n",
    "    return total_cluster_score\n",
    "    \n",
    "def majority_voting(cluster, classification_index):\n",
    "    # get the frequency of the classfication index\n",
    "    values, counts = utils.get_frequencies(cluster, classification_index)\n",
    "    \n",
    "    # get the biggest one \n",
    "    highest_freq_index = counts.index(max(counts))\n",
    "    \n",
    "    # return that classification\n",
    "    return values[highest_freq_index]\n",
    "    \n",
    "def predict(random_instance, centroids, clusters, classification_index):\n",
    "    distances = []\n",
    "    for centroid in centroids:\n",
    "        distance = compute_distance(random_instance, centroid)\n",
    "        distances.append(distance)\n",
    "    cluster_index = distances.index(min(distances))\n",
    "    \n",
    "    majority_classification = majority_voting(clusters[cluster_index], classification_index)\n",
    "    print(\"Random Instance: \", random_instance)\n",
    "    print(\"Majority Classification: \", majority_classification)\n",
    "    \n",
    "def find_best_cluster(table, minimum, maximum):\n",
    "    objective_scores = []\n",
    "    x_axis = []\n",
    "    for i in range(minimum, maximum+1):\n",
    "        print(i)\n",
    "        score, cluster_table, centroids = k_means_clustering(i, table)\n",
    "        objective_scores.append(score)\n",
    "        x_axis.append(i)\n",
    "    \n",
    "    plt.figure()\n",
    "    plt.plot(x_axis, objective_scores)\n",
    "    plt.show()\n",
    "        \n",
    "#find_best_cluster(table, 3, 10)\n",
    "#From plotting these objective functions scores, it appears that k=7 is an appropriate value.\n",
    "score, distances_table, centroids = k_means_clustering(7, table)\n",
    "# create a cluster table\n",
    "cluster_table = copy.deepcopy(table)\n",
    "for i, row in enumerate(cluster_table):\n",
    "    cluster_table[i].append(distances_table[i][-1])\n",
    "#utils.pretty_print(cluster_table)\n",
    "print(centroids)\n",
    "# group by cluster\n",
    "group_names, groups = utils.group_by(cluster_table, len(cluster_table[0])-1)\n",
    "#print(group_names)\n",
    "#utils.pretty_print(cluster_table)\n",
    "\n",
    "# predicting\n",
    "random_instance = [8.9,0.22,0.48,1.8,0.077,29,60,0.9968,3.39,0.53,9.4,6]\n",
    "predict(random_instance, centroids, groups, header.index(\"quality\"))\n",
    "#utils.pretty_print(table)\n",
    "\n",
    "for group in groups:\n",
    "    values, counts = utils.get_frequencies(group, header.index(\"quality\"))\n",
    "    avg_att_vals = compute_average(group, len(group[0])-1)\n",
    "    print(\"=\" * 60)\n",
    "    print(\"values: \", values)\n",
    "    print(\"counts: \", counts)\n",
    "    print(\"average vals:\", avg_att_vals)\n",
    "    print(\"=\" * 60)\n",
    "    #utils.pretty_print(group)\n",
    "    #values, counts = utils.get_frequencies(group, header.index(\"quality\"))\n",
    "    #print(\"=\" * 60)\n",
    "    #print(\"=\" * 60)\n",
    "    \n",
    "    # we can get the average of each attribute and see how they differ for each group\n",
    "    # total sulfur dioxide varies between each, appears lower the better\n",
    "    # free sulfur dioxide is the same, less is higher quality\n",
    "    # higher fixed acidity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## From plotting these objective functions scores, it appears that k=7 is an appropriate value.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
