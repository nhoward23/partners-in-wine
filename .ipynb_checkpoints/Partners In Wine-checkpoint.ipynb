{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Partners in Wine\n",
    "\n",
    "<img src=\"wine.png\" width=\"400\" height=\"500\">\n",
    "\n",
    "## Introduction\n",
    "\n",
    "### What is the dataset?\n",
    "This dataset is various attributes describing the makeup of the wine Portuguese \"Vinho Verde\" wine. The classification is a rating from 1-10 to decribe the quality of the wine. \n",
    "\n",
    "\n",
    "### What will we do with this dataset?\n",
    "\n",
    "Analysis: We analyzed the dataset with K-means clustering in order to discover what attributes are essential to having a quality wine.\n",
    "\n",
    "Classification: We used KNN, Naive Bayes, K-means clustering, and KNN-Bagging Ensemble classifiers on our dataset. We use these to predict the quality of a wine based on various attributes of our choosing.\n",
    "\n",
    "### What classification performed the best?\n",
    "Naive Bayes and K-means clustering were both are best classifiers with an accuracy around 80%. We hypothesize that these perform the best because the classification are distributed in a way that 5 and 6 are our most common label.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import utils\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "table = utils.read_table('red_wine_quality.csv')\n",
    "header = table[0]\n",
    "table = table[1:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Analysis\n",
    "\n",
    "Data Analysis: This section must provide details about the dataset. You must include:\n",
    "Information about the dataset itself, e.g., the attributes and attribute types, the number of instances, and the attribute being used as the label.\n",
    "Relevant summary statistics about the dataset.\n",
    "Data visualizations highlighting important/interesting aspects of your dataset. Visualizations may include frequency distributions, comparisons of attributes (scatterplot, multiple frequency diagrams), box and whisker plots, etc. The goal is not to include all possible diagrams, but instead to select and highlight diagrams that provide insight about the dataset itself.\n",
    "Note that this section must describe the above (in paragraph form) and not just provide diagrams and statistics. Also, each figure included must have a figure caption (Figure number and textual description) that is referenced from the text (e.g., “Figure 2 shows a frequency diagram for ...”).\n",
    "\n",
    "\n",
    "### About the Dataset\n",
    "\n",
    "The dataset has approximately 1600 instances. There are 12 attributes and they are all continuous. They are floats. The attribute being used as the label is the quality attribute. It is ranked on a scale from 1-10. [View the dataset here](https://archive.ics.uci.edu/ml/datasets/wine+quality). \n",
    "\n",
    "The attributes are:\n",
    "- fixed acidity\n",
    "- volatile acidity\n",
    "- citric acid\n",
    "- residual sugar\n",
    "- chlorides \n",
    "- free sulfur dioxide \n",
    "- total sulfur dioxide\n",
    "- density \n",
    "- pH\n",
    "- sulfates\n",
    "- alcohol\n",
    "- **quality**\n",
    "\n",
    "### Summary Statistics and Data Visualization\n",
    "\n",
    "Since we aren't oenologists , we don't really know much about these attributes and their relations. Since we really only care about how it relates to the quality, we will perform K-means clustering to analyze the data. We will cluster them and see what the majority classification is for that cluster. Then, we will get the average of each attribute and see if we can spot any differences between these clusters. \n",
    "\n",
    "However, before we do that, it is important to view the distribution of our classifications. This will give us an insight in our future classification predictions. \n",
    "\n",
    "#### No. of quality ratings:\n",
    "As you can see in Figure 1, the majority of the dataset is classified as either a 5 or 6. There are some 3's, 7's, and 8's but no 9 or 10. This distribution is important to be aware of when we start our classifications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "values, counts = utils.get_frequencies(table, header.index('quality'))\n",
    "plt.figure()\n",
    "plt.bar(values, counts)\n",
    "plt.title(\"Figure 1.\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### K-Means Clustering to help with Data Analysis \n",
    "\n",
    "We can use k-means clustering to determine what attributes make for a high quality wine. K-means clustering groups up instances that are closest to eachother with k groups. This image is an example:\n",
    "\n",
    "<img src=\"cluster.png\" width=\"400\" height=\"500\">\n",
    "\n",
    "The code below is used to find the best k for our dataset. We are going to try k=3 to k=10. We will plot the objective function scores in order to find the \"elbow\" point, the point at which the total sum of squares drops less drastically. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n",
      "20181.164137619082\n",
      "4\n",
      "17360.11819202622\n",
      "5\n",
      "15022.907290461892\n",
      "6\n",
      "13449.40049327012\n",
      "7\n",
      "12606.12490152986\n",
      "8\n",
      "12017.666899775348\n",
      "9\n",
      "11415.713148442654\n",
      "10\n",
      "10972.850957097786\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import random\n",
    "import math\n",
    "import copy\n",
    "import numpy\n",
    "import matplotlib.pyplot as plt\n",
    "import k_means\n",
    "\n",
    "def k_means_clustering(k, table):\n",
    "    # Select k objects in an arbitrary fashion. Use these as the initial set of k centroids.\n",
    "    centroids = k_means.select_init_centroids(k, table)\n",
    "    \n",
    "    match = False\n",
    "    while match == False: \n",
    "        \n",
    "        # Compute the distances of each instance to each centroid\n",
    "        distances_table = [ [] for i in range(len(table)) ]\n",
    "        for centroid in centroids:\n",
    "            k_means.append_distance(table, distances_table, centroid)\n",
    "\n",
    "        # Find the biggest distance and assign instance to that centroid\n",
    "        k_means.find_centroid(distances_table)\n",
    "\n",
    "        # Recalculate the centroids by getting the mean of each cluster\n",
    "        new_centroids = k_means.recalculate_centroids(table, distances_table, [])\n",
    "\n",
    "        # Check to see if the centroids have converged\n",
    "        match = k_means.compare_centroids(centroids, new_centroids)\n",
    "        centroids = new_centroids\n",
    "\n",
    "    # Now we know what instance belongs to what cluster\n",
    "    # table and distances are \"parallel tables\" \n",
    "    score = objective_function(table, distances_table, centroids)\n",
    "    return score, distances_table, centroids\n",
    "    \n",
    "def objective_function(table, cluster_table, centroids):\n",
    "    # Combine and group by cluster \n",
    "    new_table = k_means.combine_tables(table, cluster_table)\n",
    "    cluster_index = len(new_table[0])-1\n",
    "    group_names, groups = utils.group_by(new_table, cluster_index)\n",
    "    \n",
    "    # for each cluster, compute the sum of squares\n",
    "    # add these to a cluster_total of all clusters\n",
    "    total_cluster_score = 0\n",
    "    for i, cluster in enumerate(groups):\n",
    "        distances = []\n",
    "        for row in cluster:\n",
    "            distance = k_means.compute_distance(row, centroids[row[cluster_index]])\n",
    "            distances.append(distance)\n",
    "        total_cluster_score += sum(distances)\n",
    "    print(total_cluster_score)\n",
    "    return total_cluster_score\n",
    "        \n",
    "def find_best_cluster(table, minimum, maximum):\n",
    "    objective_scores = []\n",
    "    x_axis = []\n",
    "    for i in range(minimum, maximum+1):\n",
    "        print(i)\n",
    "        score, cluster_table, centroids = k_means_clustering(i, table)\n",
    "        objective_scores.append(score)\n",
    "        x_axis.append(i)\n",
    "    \n",
    "    plt.figure()\n",
    "    plt.title(\"Figure 2.\")\n",
    "    plt.plot(x_axis, objective_scores)\n",
    "    plt.show()\n",
    "\n",
    "# find the best k for clustering from 3-10.\n",
    "find_best_cluster(table, 3, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Best K is 6! \n",
    "\n",
    "Each time this runs, the values change a little bit but it is roughly the same output everytime. The elbow point appears to be around 6 or 7 everytime, as you can see in Figure 2. We will choose to use 6 clusters. \n",
    "\n",
    "Now that we have found the best number of clusters, we can use them to learn more information about our dataset. \n",
    "We will look at the cluster and see what we can find out by observing the averages of each attribute for each cluster. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13449.40049327012\n",
      "============================================================\n",
      "Cluster 1:\n",
      "============================================================\n",
      "Most Frequent Classification:  6.0\n",
      "\n",
      "Averages for Each Attribute \n",
      "fixed acidity:  8.67\n",
      "volatile acidity:  0.51\n",
      "citric acid:  0.3\n",
      "residual sugar:  2.51\n",
      "chlorides:  0.09\n",
      "free sulfur dioxide:  6.08\n",
      "total sulfur dioxide:  15.15\n",
      "density:  1.0\n",
      "pH:  3.3\n",
      "sulphates:  0.64\n",
      "alcohol:  10.72\n",
      "quality:  5.76\n",
      "============================================================\n",
      "\n",
      "============================================================\n",
      "Cluster 2:\n",
      "============================================================\n",
      "Most Frequent Classification:  5.0\n",
      "\n",
      "Averages for Each Attribute \n",
      "fixed acidity:  8.06\n",
      "volatile acidity:  0.56\n",
      "citric acid:  0.32\n",
      "residual sugar:  3.45\n",
      "chlorides:  0.09\n",
      "free sulfur dioxide:  32.28\n",
      "total sulfur dioxide:  139.06\n",
      "density:  1.0\n",
      "pH:  3.23\n",
      "sulphates:  0.7\n",
      "alcohol:  9.8\n",
      "quality:  5.13\n",
      "============================================================\n",
      "\n",
      "============================================================\n",
      "Cluster 3:\n",
      "============================================================\n",
      "Most Frequent Classification:  5.0\n",
      "\n",
      "Averages for Each Attribute \n",
      "fixed acidity:  8.15\n",
      "volatile acidity:  0.52\n",
      "citric acid:  0.28\n",
      "residual sugar:  2.48\n",
      "chlorides:  0.09\n",
      "free sulfur dioxide:  25.55\n",
      "total sulfur dioxide:  65.09\n",
      "density:  1.0\n",
      "pH:  3.33\n",
      "sulphates:  0.68\n",
      "alcohol:  10.27\n",
      "quality:  5.55\n",
      "============================================================\n",
      "\n",
      "============================================================\n",
      "Cluster 4:\n",
      "============================================================\n",
      "Most Frequent Classification:  6.0\n",
      "\n",
      "Averages for Each Attribute \n",
      "fixed acidity:  8.35\n",
      "volatile acidity:  0.52\n",
      "citric acid:  0.26\n",
      "residual sugar:  2.48\n",
      "chlorides:  0.09\n",
      "free sulfur dioxide:  18.3\n",
      "total sulfur dioxide:  45.16\n",
      "density:  1.0\n",
      "pH:  3.32\n",
      "sulphates:  0.67\n",
      "alcohol:  10.45\n",
      "quality:  5.7\n",
      "============================================================\n",
      "\n",
      "============================================================\n",
      "Cluster 5:\n",
      "============================================================\n",
      "Most Frequent Classification:  6.0\n",
      "\n",
      "Averages for Each Attribute \n",
      "fixed acidity:  8.32\n",
      "volatile acidity:  0.53\n",
      "citric acid:  0.25\n",
      "residual sugar:  2.22\n",
      "chlorides:  0.08\n",
      "free sulfur dioxide:  11.6\n",
      "total sulfur dioxide:  28.85\n",
      "density:  1.0\n",
      "pH:  3.32\n",
      "sulphates:  0.65\n",
      "alcohol:  10.45\n",
      "quality:  5.72\n",
      "============================================================\n",
      "\n",
      "============================================================\n",
      "Cluster 6:\n",
      "============================================================\n",
      "Most Frequent Classification:  5.0\n",
      "\n",
      "Averages for Each Attribute \n",
      "fixed acidity:  7.8\n",
      "volatile acidity:  0.58\n",
      "citric acid:  0.26\n",
      "residual sugar:  3.1\n",
      "chlorides:  0.09\n",
      "free sulfur dioxide:  23.2\n",
      "total sulfur dioxide:  95.5\n",
      "density:  1.0\n",
      "pH:  3.31\n",
      "sulphates:  0.64\n",
      "alcohol:  10.09\n",
      "quality:  5.39\n",
      "============================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# cluster the dataset to form 6 groups \n",
    "score, distances_table, centroids = k_means_clustering(6, table)\n",
    "\n",
    "# create a cluster table\n",
    "cluster_table = copy.deepcopy(table)\n",
    "for i, row in enumerate(cluster_table):\n",
    "    cluster_table[i].append(distances_table[i][-1])\n",
    "\n",
    "# group by cluster \n",
    "group_names, groups = utils.group_by(cluster_table, len(cluster_table[0])-1)\n",
    "\n",
    "# for each cluster, let's find out some stuff.\n",
    "for i, group in enumerate(groups):\n",
    "    values, counts = utils.get_frequencies(group, header.index(\"quality\"))\n",
    "    avg_att_vals = k_means.compute_average(group, len(group[0])-1)\n",
    "    print(\"=\" * 60)\n",
    "    print(\"Cluster \" + str(i+1) + \":\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # get the most frequently occuring quality classification\n",
    "    most_freq_qual_index = counts.index(max(counts))\n",
    "    most_freq_quality = values[most_freq_qual_index]\n",
    "    print(\"Most Frequent Classification: \", most_freq_quality)\n",
    "    print()\n",
    "    \n",
    "    # print the average for each attribute\n",
    "    print(\"Averages for Each Attribute \")\n",
    "    for i, attribute in enumerate(avg_att_vals):\n",
    "        print(header[i] + \": \", attribute)\n",
    "    \n",
    "    print(\"=\" * 60)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What can we take away? \n",
    "\n",
    "By observing the averages for each attribute in each cluster, we have discovered some attributes that vary. We noticed that the clusters with the most classifications of a quality 6 have some attributes that vary from clusters that are mostly a 5 rating. \n",
    "\n",
    "Classification 6 has:\n",
    "- a slightly higher fixed acidity\n",
    "- a lower residual sugar\n",
    "- a lower free sulfur dioxide\n",
    "- a lower total sulfur dioxide\n",
    "\n",
    "We can inform the vineyard of this information and to carefully monitor these properties because it appears to have an affect on the quality.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification Results\n",
    "\n",
    "\n",
    "## Random Subsampling with K-Means Clustering\n",
    "We used the centroids and clusters we created above to make predictions on random instances. We did this by taking random instances and finding what centroid it is closest to. Then, we find the majority class label of that cluster with majority voting. We tested our classifier by using random subsampling. For k times, we create a new training set and recalculated the centroids and clusters. Then we predict all the instances in the test set and compute the accuracy. You can see below that this classifier has a predictive accuracy of 83%. This is a high-performing classifier. I think that this is a high performer because most often the majority class of a cluster will be a 5 or 6. Since most of the instances in our dataset have a quality rating of 5 or 6 it will predict correctly for instances a good number of times. Below we run the k_means clustering with random subsampling and show its accuracy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================================\n",
      "Predictive Accuracy of K-Means Clustering with Random Subsampling\n",
      "============================================================================\n",
      "Accuracy = 0.80, error rate = 0.20\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#random subsampling \n",
    "random_avg_accuracy, random_stderr = k_means.random_subsampling(5, table, header.index(\"quality\"), centroids, groups)\n",
    "print(\"=\" * 76)\n",
    "print(\"Predictive Accuracy of K-Means Clustering with Random Subsampling\")\n",
    "print(\"=\" * 76)\n",
    "print(\"Accuracy = %0.2f, error rate = %0.2f\" %(random_avg_accuracy, random_stderr))\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive Bayes with Random Subsampling\n",
    "The next classifier we used is Naive Bayes. This classifier uses prior and posterior probabilities and selects the classification with the highest probability to predict the label for each instance. It was designed to take in a list of indicies to include as posterior probabilities. We tested the classifier's predictive ability by using random subsampling. With this testing method, it has an accuracy of around 80%, fluctuating depending on what attributes you choose. \n",
    "\n",
    "Since we discerned that the fixed acidity, residual sugar, free sulfur dioxide, and total sulfur dioxide may result in a higher quality wine, we can test these out with our naive bayes classifier to see these attributes are more predictive of the quality of wine. The results show that the accuracy with these attributes are 81%. \n",
    "\n",
    "We also will test some other attributes, including residual sugar, density, and alcohol. We believe that college students will be more concerned with these attributes so we want to see what will be predicted by using these attributes. The results show that using Naive Bayes with these attributes results in an accuracy of 84%. \n",
    "\n",
    "While the second set of attributes had the highest accuracy, both are in the 80 percentile which is high. Like the classifier above it predicts 5 or 6 most of the time because the majority of the dataset is classified as 5 or 6 so the predictive accuracy is high. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================================\n",
      "Predictive Accuracy of Naive Bayes with Guassian\n",
      "============================================================================\n",
      "Random Subsample (k=10, 2:1 Train/Test)\n",
      "Naive Bayes: accuracy = 0.82, error rate = 0.18\n",
      "\n",
      "============================================================================\n",
      "Predictive Accuracy of Naive Bayes with Guassian\n",
      "============================================================================\n",
      "Random Subsample (k=10, 2:1 Train/Test)\n",
      "Naive Bayes: accuracy = 0.85, error rate = 0.15\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import naive_bayes\n",
    "\n",
    "# we noticed these attributes vary with K-means so lets see \n",
    "indices = [header.index(\"fixed acidity\"),header.index('residual sugar'),header.index('free sulfur dioxide'), header.index('total sulfur dioxide')]\n",
    "#['fixed acidity', 'volatile acidity', 'citric acid', 'residual sugar', 'chlorides', 'free sulfur dioxide',\n",
    "#'total sulfur dioxide', 'density', 'pH', 'sulphates', 'alcohol', 'quality']\n",
    "\n",
    "# Random SubSampling with Naive Bayes\n",
    "random_avg_accuracy, random_stderr = naive_bayes.random_subsampling(5, table, header.index(\"quality\"), indices)\n",
    "print(\"=\" * 76)\n",
    "print(\"Predictive Accuracy of Naive Bayes with Guassian\")\n",
    "print(\"=\" * 76)\n",
    "print(\"Random Subsample (k=10, 2:1 Train/Test)\")\n",
    "print(\"Naive Bayes: accuracy = %0.2f, error rate = %0.2f\" %(random_avg_accuracy, random_stderr))\n",
    "print()\n",
    "\n",
    "\n",
    "# college students care about these\n",
    "indices = [header.index('residual sugar'),header.index('density'), header.index('alcohol')]\n",
    "\n",
    "\n",
    "# Random SubSampling with Naive Bayes\n",
    "random_avg_accuracy, random_stderr = naive_bayes.random_subsampling(5, table, header.index(\"quality\"), indices)\n",
    "print(\"=\" * 76)\n",
    "print(\"Predictive Accuracy of Naive Bayes with Guassian\")\n",
    "print(\"=\" * 76)\n",
    "print(\"Random Subsample (k=10, 2:1 Train/Test)\")\n",
    "print(\"Naive Bayes: accuracy = %0.2f, error rate = %0.2f\" %(random_avg_accuracy, random_stderr))\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KNN Using Random Subsampling\n",
    "Using random subsampling, we can randomly select a train and test set and train/test our classifier on those sets. We repeat this process k times and take the average accuracy. The current attributes being tested on are density, sugar, and alcohol. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================================\n",
      "Predictive Accuracy of KNN using Random Subsampling\n",
      "============================================================================\n",
      "Random Subsample (k=10, 2:1 Train/Test)\n",
      "accuracy: 0.6941176470588235\n",
      "error rate: 0.3058823529411765\n"
     ]
    }
   ],
   "source": [
    "data = table[:100]\n",
    "utils.knn_random_subsampling(data, 5, [3, 7, 10], 11, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ensemble Classification Using Bootstrap Aggregation on KNN\n",
    "With this classification approach, we first split our dataset into 1/3 testing and 2/3 remainder. Then, the remainder set is split into 2/3 training and 1/3 validation. For each of the k canditate classifiers, we train the classifier on our training set, and then determine it's accuracy over the validation set. We only use this classifier if it's accuracy is over a certain threshold. If it qualifies, it get's added to the ensemble.\n",
    "\n",
    "Each classifier is trained and validated over a different set of random data collected using bagging. The bagging method involves taking our remaining data (size N), and randomly selecting N instances from this data (with replacement) to form our training set. Many instances will be selected more than once, and some not at all. Whatever isn't selected is put into the validation set. \n",
    "\n",
    "The current attributes being tested on are density, sugar, and alcohol. The minimum accuracy is set to 60%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 0.6060606060606061\n",
      "error rate: 0.3939393939393939\n"
     ]
    }
   ],
   "source": [
    "import ensemble_classification as es\n",
    "\n",
    "def bootstrap_aggregation(table, num_class, attribute_indices, classification_index, min_acc, k):\n",
    "    # runs classifiers over instances in the test set to produce predictions\n",
    "    full_test = utils.generate_test_set(table)\n",
    "    test_classifications = utils.get_classifications(full_test, classification_index)\n",
    "    test = utils.normalize_instances(full_test, attribute_indices)\n",
    "    # each approved training set and the accuracy of each said set\n",
    "    training, accuracies = es.select_approved_training_sets(table, num_class, attribute_indices, classification_index, min_acc, k)\n",
    "    correct = 0\n",
    "    if(len(training) == 0):\n",
    "        print(\"ensemble is empty\")\n",
    "    else:\n",
    "        for index, sample in enumerate(test):\n",
    "            for train_set in training:\n",
    "                predictions = []\n",
    "                # generate the prediction for this training set\n",
    "                prediction = es.classifier_prediction(train_set, attribute_indices, classification_index, sample, k)\n",
    "                predictions.append(prediction)\n",
    "            # generate a final prediction from all classifiers using weighted majority voting\n",
    "            final_prediction = utils.generate_weighted_majority_prediction(predictions, accuracies, test_classifications)\n",
    "            full_sample = full_test[index]\n",
    "            actual_classification = full_sample[classification_index]\n",
    "            if (final_prediction == actual_classification):\n",
    "                correct+= 1\n",
    "        accuracy = correct/len(test)\n",
    "        print(\"accuracy: \"+str(accuracy))\n",
    "        print(\"error rate: \"+str(1-accuracy))\n",
    "# use a subset of the dataset otherwise it takes a long time to run\n",
    "data = table[:100]\n",
    "bootstrap_aggregation(data, 20, [3, 7, 10], 11, .6, 5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "While their are so many ways to combine the data set we tested on, all the instances of the set were relatively similar. They had relatively similar values for each attribute and almost all of them fell into the score category of 4, 5, 6 or 7. For that reason, we found that our classifiers had a really hard time making accurate predictions. The accuracy for KNN using random subsampling as well as the accuracy for our ensemble classification were both pretty low, or at least lower than anticipated. However, what we noticed of the predictions was that our classification methods struggled on wines that were very similar. Most incorrect classifications classified a 5 as a 6 or a 6 as a 5. Because of the similarities among the instances in our dataset, our classification methods struggled.\n",
    "\n",
    "We tested of a variety of different attributes. After using our clustering method, we learned that the attributes fixed acidity, residual sugar, free sulfur dioxide, total sulfur dioxide might have a stronger impact on our classification accuracy. We found this to be true in other cases, like KNN using random subsampling. Initally, with 4 selected random attributes, the accuracy was in the high 50s, low 60s. Using the four attributes previously mentioned, we were able to raise the accuracy to high 60s. However, in the case of our ensemble classification, testing on these attributed did not help our classifiers any more than any other random selection of attributes. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
